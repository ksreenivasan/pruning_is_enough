=> Reading YAML config from configs/hypercube/resnet20/resnet20_quantized_hypercube_reg_bottom_K.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='resnet20', batch_size=128, bias=False, bn_type='NonAffineBatchNorm', checkpoint_at_prune=True, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/resnet20/resnet20_quantized_hypercube_reg_bottom_K.yml', conv_type='SubnetConv', data='data/datasets/', data_dir='../data', dataset='CIFAR10', dist_backend='nccl', epochs=150, evaluate=False, evaluate_only=False, fine_tune_lr=0.01, fine_tune_optimizer='sgd', fine_tune_wd=0.0001, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=3, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', init='signed_constant', interpolate='prob', iter_period=20, iter_start=0, label_smoothing=None, last_layer_dense=False, lmbda=1e-06, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=0.05, lr_adjust=50, lr_gamma=0.1, lr_policy='cosine_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.9, multiprocessing_distributed=False, name='resnet20_cifar10_sc_hypercube_reg_bottom_K', nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_epochs=10, num_round=1, num_test=1, num_trial=1, num_workers=4, optimizer='adam', plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, prune_rate=0.2, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='var_red_1', reinit=False, results_filename=None, resume=None, round='naive', save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='skew', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=False, skip_sanity_checks=False, start_epoch=None, start_from_nothing=False, submask_size=1, td=0.99, temp=100000, trainer='default', trial_num=1, warmup_length=0, wd=0.0, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
=> Using trainer from trainers.default
Use GPU: 3 for training
=> Creating model 'resnet20'
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
==> Building first layer with <class 'utils.conv_type.SubnetConv'>
tensor(-11.9753)
tensor(-2.1213)
tensor(5.1854)
tensor(-6.1283)
tensor(9.6638)
tensor(-1.4142)
tensor(-2.8284)
tensor(-9.6638)
tensor(9.1667)
tensor(-1.3333)
tensor(-4.3333)
tensor(-5.1667)
tensor(8.1667)
tensor(-10.0000)
tensor(11.3137)
tensor(-14.3778)
tensor(-8.8388)
tensor(-0.5893)
tensor(-5.7747)
tensor(3.1820)
==> Setting prune rate of network to 0.2
==> Setting prune rate of conv1 to 0.2
==> Setting prune rate of layer1.0.conv1 to 0.2
==> Setting prune rate of layer1.0.conv2 to 0.2
==> Setting prune rate of layer1.1.conv1 to 0.2
==> Setting prune rate of layer1.1.conv2 to 0.2
==> Setting prune rate of layer1.2.conv1 to 0.2
==> Setting prune rate of layer1.2.conv2 to 0.2
==> Setting prune rate of layer2.0.conv1 to 0.2
==> Setting prune rate of layer2.0.conv2 to 0.2
==> Setting prune rate of layer2.1.conv1 to 0.2
==> Setting prune rate of layer2.1.conv2 to 0.2
==> Setting prune rate of layer2.2.conv1 to 0.2
==> Setting prune rate of layer2.2.conv2 to 0.2
==> Setting prune rate of layer3.0.conv1 to 0.2
==> Setting prune rate of layer3.0.conv2 to 0.2
==> Setting prune rate of layer3.1.conv1 to 0.2
==> Setting prune rate of layer3.1.conv2 to 0.2
==> Setting prune rate of layer3.2.conv1 to 0.2
==> Setting prune rate of layer3.2.conv2 to 0.2
==> Setting prune rate of fc to 0.2
=> Rough estimate model params 429322
=> Freezing model weights
==> No gradient to conv1.weight
==> No gradient to layer1.0.conv1.weight
==> No gradient to layer1.0.conv2.weight
==> No gradient to layer1.1.conv1.weight
==> No gradient to layer1.1.conv2.weight
==> No gradient to layer1.2.conv1.weight
==> No gradient to layer1.2.conv2.weight
==> No gradient to layer2.0.conv1.weight
==> No gradient to layer2.0.conv2.weight
==> No gradient to layer2.1.conv1.weight
==> No gradient to layer2.1.conv2.weight
==> No gradient to layer2.2.conv1.weight
==> No gradient to layer2.2.conv2.weight
==> No gradient to layer3.0.conv1.weight
==> No gradient to layer3.0.conv2.weight
==> No gradient to layer3.1.conv1.weight
==> No gradient to layer3.1.conv2.weight
==> No gradient to layer3.2.conv1.weight
==> No gradient to layer3.2.conv2.weight
==> No gradient to fc.weight
Traceback (most recent call last):
  File "main.py", line 1286, in <module>
    main()
  File "main.py", line 280, in main
    main_worker(parser_args.gpu, ngpus_per_node)
  File "main.py", line 308, in main_worker
    model = set_gpu(parser_args, model)
  File "main.py", line 1047, in set_gpu
    model.cuda(parser_args.gpu)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 637, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 552, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 637, in <lambda>
    return self._apply(lambda t: t.cuda(device))
KeyboardInterrupt
