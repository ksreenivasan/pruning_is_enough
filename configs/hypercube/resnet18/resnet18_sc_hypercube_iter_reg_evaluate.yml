# Hypercube optimization
algo: 'hc_iter'
iter_period: 75
evaluate: True
pretrained: 'runs/resnet18_sc_hypercube_iter_reg/resnet18_cifar10_hypercube_iter_reg/prune_rate=0.0/4/checkpoints/model_best.pth'
#pretrained: 'runs/resnet18_sc_hypercube_iter_reg/resnet18_cifar10_hypercube_iter_reg/prune_rate=0.0/3/checkpoints/model_best.pth'



# Architecture
arch: cResNet18

# ===== Dataset ===== #
dataset: CIFAR10
name: resnet18_cifar10_hypercube_iter_reg

# ===== Learning Rate Policy ======== #
optimizer: adam
lr: 0.05
lr_policy: cosine_lr
#lr_policy: multistep_lr
#lr_gamma: 0.5
#lr_adjust: 30 #30


# ===== Network training config ===== #
epochs: 150
weight_decay: 0.0 #0.0001 # 0.0
momentum: 0.9
batch_size: 128

# ===== Sparsity =========== #
conv_type: SubnetConv
bn_type: NonAffineBatchNorm
freeze_weights: True
prune_rate: 0.0
init: signed_constant
score_init: unif
scale_fan: False #True


# ===== Rounding ===== #
round: naive
noise: True
noise_ratio: 0


# ===== Regularization ===== #
regularization: var_red_2
lmbda: 0.001 #0.000001


# ===== Hardware setup ===== #
workers: 4
gpu: 3

