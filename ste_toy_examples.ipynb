{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emotional-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for experiment\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # making sure GPU runs are deterministic even if they are slower\n",
    "    print(\"Seeded everything: {}\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded everything: 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-jungle",
   "metadata": {},
   "source": [
    "# Test how the straight through estimator works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-mailman",
   "metadata": {},
   "source": [
    "$$\\text{With indicators:}\\quad f(x_1, x_2) = \\mathbb{1}_{x_1 > 5}x_1^2 + \\mathbb{1}_{x_2 > 5}x_2^2$$\n",
    "$$\\text{Without indicators:}\\quad g(x_1, x_2) = x_1^2 + x_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x > 5}\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        # this is the correct way to do it (torch.gte also works)\n",
    "        mask = Variable((x > 5).float(), requires_grad=True)\n",
    "        # if you do this: mask = (x > 5).int(), then pytorch breaks the computational graph\n",
    "        # and no longer calls .backward()!\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # NOTE: This isn't getting printed! I don't know why\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-reviewer",
   "metadata": {},
   "source": [
    "## See if the indicator function works as expected (it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enclosed-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1, 6])\n",
    "GetIndicators.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boolean-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    out = (mask * x** 2).sum()\n",
    "    return out\n",
    "\n",
    "def g(x):\n",
    "    out = (x**2).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-defense",
   "metadata": {},
   "source": [
    "$$\\nabla g(x) = \\begin{bmatrix} 2x_1\\\\ 2x_2 \\end{bmatrix}$$\n",
    "\n",
    "### Assuming STE\n",
    "$$\\frac{\\partial f(x)}{\\partial x_i} = \\mathbb{1}_{x_i > 5}\\frac{\\partial x_i^2}{x_i} + x_i^2 \\frac{\\partial \\mathbb{1}_{x_i > 5}}{\\partial x_i} = 2x_i\\mathbb{1}_{x_i > 5} + x_i^2 \\frac{\\partial \\mathbb{1}_{x_i > 5}}{\\partial x_i} = 2x_i\\mathbb{1}_{x_i > 5} + x_i^2$$\n",
    "\n",
    "#### (The grad of indicator is just 1 for STE. If you make it 0 whenever the indicator is off, that is not STE, but may also be interesting!)\n",
    "\n",
    "### Now check if that is what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming grad = Outgoing grad = tensor([ 4., 36.])\n",
      "grad(f(x)) = tensor([ 4., 48.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parallel-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(g(x)) = tensor([ 4., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "g_out = g(x)\n",
    "g_out.backward()\n",
    "print(\"grad(g(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-assist",
   "metadata": {},
   "source": [
    "## It works as expected now! STE is a little weird, but it is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-priest",
   "metadata": {},
   "source": [
    "# Now, try a more typical usage. Threshold of a function like Bengio's example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-shell",
   "metadata": {},
   "source": [
    "$$f(x_1, x_2) = \\mathbf{1}^T \\mathbb{1}_{x^2 > 25} = \\mathbb{1}_{x_1^2 > 25} + \\mathbb{1}_{x_2^2 > 25}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "toxic-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x^2 > 25} (which is actually the same as x > 5)\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        mask = Variable((x**2 > 25).float(), requires_grad=True)\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        return g\n",
    "\n",
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    # if I don't include a .float(), then I can't set requires_grad=True\n",
    "    out = mask.sum().float()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "magnetic-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming grad = Outgoing grad = tensor([1., 1.])\n",
      "grad(f(x)) = tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-shark",
   "metadata": {},
   "source": [
    "## Gradient is just 1. As expected, it ignores the Indicator in the backward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-property",
   "metadata": {},
   "source": [
    "# Let's see how this plays with clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-mortgage",
   "metadata": {},
   "source": [
    "$$ f(x_1, x_2) = \\mathbf{1}^T((P_{[1, 4]}(x))^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enormous-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = torch.clamp(x, 1, 4)\n",
    "    out = (x**2).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "judicial-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x)) = tensor([4., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-export",
   "metadata": {},
   "source": [
    "## As expected, any projected value just gets 0 gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-valve",
   "metadata": {},
   "source": [
    "## The right way to project imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "motivated-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x)) = tensor([4., 8.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "with torch.no_grad():\n",
    "    x = torch.clamp(x, 1, 4)\n",
    "# need to reset requires_grad\n",
    "x.requires_grad=True\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-canadian",
   "metadata": {},
   "source": [
    "## EP's impelementation. Turns out their STE works the right way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "trained-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x > 5}\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        out = x.clone()\n",
    "\n",
    "        # flat_out and out access the same memory.\n",
    "        flat_out = out.flatten()\n",
    "        flat_out[0] = 0\n",
    "        flat_out[1] = 1\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # NOTE: This isn't getting printed! I don't know why\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occasional-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming grad = Outgoing grad = tensor([ 4., 36.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    out = (mask * x**2).sum()\n",
    "    return out\n",
    "\n",
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "f_out = f(x)\n",
    "f_out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-winner",
   "metadata": {},
   "source": [
    "## Trying out our implementation of HC. That is using STE the right way too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "japanese-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x > 5}\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        out = torch.gt(x, torch.ones_like(x)*5).int().float()\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # NOTE: This isn't getting printed! I don't know why\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "banned-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming grad = Outgoing grad = tensor([ 4., 36.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    out = (mask * x**2).sum()\n",
    "    return out\n",
    "\n",
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "f_out = f(x)\n",
    "f_out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-israel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
